{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building First Ai Agent with Reinforcement Learning\n",
    "- Explore Gymnasium ( openai gym )\n",
    "- Implement Q-learning algorithm using gymnasium\n",
    "- Try to implement perform two tasks with algorithm designed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install gymnasium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "import time\n",
    "import random\n",
    "from IPython import display\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate some descriptive names to display for the actions\n",
    "\n",
    "action_desc = {\n",
    "    0 : \"Move south (down)\",\n",
    "    1 : \"Move north (up)\",\n",
    "    2 : \"Move east (right)\",\n",
    "    3 : \"Move west (left)\",\n",
    "    4 : \"Pickup Passenger\",\n",
    "    5 : \"Drop off passenger\" \n",
    "}\n",
    "\n",
    "# Create the Taxi environment\n",
    "env = gym.make('Taxi-v3', render_mode = \"rgb_array\")\n",
    "\n",
    "# Initilize the environment and draw the current state\n",
    "obs = env.reset()[0]\n",
    "plt.imshow(env.render())\n",
    "plt.show()\n",
    "\n",
    "# Loop for 100 steps\n",
    "for i in range(100):\n",
    "\n",
    "    # Select random action\n",
    "    action = env.action_space.sample()\n",
    "\n",
    "    # Apply the action, then observe the state and reward\n",
    "    obs, reward, done, info, other = env.step(action)\n",
    "\n",
    "    # Draw the new state\n",
    "    display.clear_output(wait = True)\n",
    "    plt.imshow(env.render())\n",
    "\n",
    "\n",
    "    # Add a caption indicating the current state, action, and reward\n",
    "    rect = matplotlib.patches.Rectangle((150,0), 250, 75, facecolor = \"#999999\", edgecolor=\"#000000\")\n",
    "    ax = plt.gca()\n",
    "    ax.add_patch(rect)\n",
    "    plt.text(165, 25, f\"State: {obs}\")\n",
    "    plt.text(165, 45, f\"Action: {action_desc[action]}\")\n",
    "    plt.text(165, 65, f\"Reward: {reward}\")\n",
    "    plt.show()\n",
    "\n",
    "# Close the environment\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How can we learn the correct sequence of actions to perform hust by testing out different actions and observing rewards?\n",
    "____\n",
    "We will introduce the concept Q-learning by looking at a simplified version of the algorithm on a much simpler version of the taxi problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_grid_state(s, n, Q):\n",
    "    \"\"\"\n",
    "    This function will be used to visualize the current state and\n",
    "    Q table for the simplified taxi example.\n",
    "    \"\"\"\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    fig.set_size_inches(12, 2)\n",
    "\n",
    "    for i in range(n):\n",
    "        if i == n-1:\n",
    "            facecolor = \"#00ff00\"\n",
    "        else:\n",
    "            facecolor = \"#ffffff\"\n",
    "        rect = matplotlib.patches.Rectangle((20*i, 0), 20, 20, facecolor=facecolor, edgecolor=\"#000000\")\n",
    "\n",
    "        ax.add_patch(rect)\n",
    "        if (i,0) in Q:\n",
    "            plt.text(20*i + 5, 30, \"<-- {}\".format(Q[(i, 0)]))\n",
    "            plt.text(20*i + 5, 24, \"{} -->\".format(Q[(i, 1)]))\n",
    "    plt.xlim([-5, 20*n+5])\n",
    "    plt.ylim([-5, 30])\n",
    "\n",
    "    ax.set_aspect(\"equal\", adjustable = 'box')\n",
    "    ax.axis('off')\n",
    "\n",
    "    taxi1 = matplotlib.patches.Rectangle((20*s+3, 5), 14, 7, facecolor=\"#ffff00\", edgecolor=\"#000000\")\n",
    "    taxi2 = matplotlib.patches.Circle((20*s+6, 5), 2, facecolor=\"#000000\", edgecolor=\"#000000\")\n",
    "    taxi3 = matplotlib.patches.Circle((20*s+14, 5), 2, facecolor=\"#000000\", edgecolor=\"#000000\")\n",
    "\n",
    "    ax.add_patch(taxi1)\n",
    "    ax.add_patch(taxi2)\n",
    "    ax.add_patch(taxi3)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_grid_state(2, 15, {})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "goal is to navigate the taxi on 1-dimensional grid to the green square on the right end of the grid. Two actions that can perform \"move right\", \"move left\"\n",
    "We incur a cost of 1 unit (or reward of -1 unit) for every step we take prior to reaching the green square\n",
    "\n",
    "## Let's implement simplified Q-learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the grid containing 15 cells\n",
    "n = 15\n",
    "# initialize the state of the left most cell\n",
    "s= 0\n",
    "# Create an empty Q table\n",
    "Q = {}\n",
    "\n",
    "\n",
    "# Initialize the Q values for the initial state\n",
    "Q[(s, 0)] = 0\n",
    "Q[(s, 1)] = 0\n",
    "\n",
    "# Loop over each 15 episodes\n",
    "# Each episode ends when the taxi reached the green cell\n",
    "episodes = 0\n",
    "while episodes < 15:\n",
    "    time.sleep(0.1)\n",
    "\n",
    "    # Display the current state\n",
    "    display.clear_output(wait = True)\n",
    "    plot_grid_state(s, 15, Q)\n",
    "\n",
    "    # input()\n",
    "\n",
    "    # If we reached the goal, re-initialize the state\n",
    "    if s == n-1:\n",
    "        time.sleep(1)\n",
    "        episodes += 1\n",
    "        s = 0\n",
    "\n",
    "    # Select an action\n",
    "    if Q[(s,0)] < Q[(s, 1)]:\n",
    "        a = 0\n",
    "    elif Q[(s,0)] > Q[(s,1)]:\n",
    "        a = 1\n",
    "    else:\n",
    "        a = random.randint(0, 1)\n",
    "\n",
    "    # Update our position\n",
    "    if a == 0:\n",
    "        s_next = max(0, s-1)\n",
    "    else:\n",
    "        s_next = min(s+1, n-1)\n",
    "    \n",
    "    # Add Q_next to the table if not yet in the table\n",
    "    if (s_next, 0) not in Q:\n",
    "        Q[(s_next, 0)] = 0\n",
    "        Q[s_next, 1] = 0\n",
    "\n",
    "    # Update the Q table\n",
    "    Qmin = min(Q[s_next, 0], Q[(s_next, 1)])\n",
    "\n",
    "    Q[(s, a)] = 1 + Qmin\n",
    "\n",
    "    # Set the current state to be the next observed state\n",
    "\n",
    "    s = s_next\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- Cost making a step with avilable options of performing left or right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearner:\n",
    "    \"\"\" This class will allow us to specify a Gymnasium environment and apply\n",
    "    tabular Q-learning on it.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, environment, g = 0.98, a=0.05, e = 0.05):\n",
    "        # Initialize the following:\n",
    "        # g : The discount factor used in our total discounted reward\n",
    "        # a : The learning rate for Q-learning\n",
    "        # e : The epsilon for epsilon-greedy action selection\n",
    "        self.g = g\n",
    "        self.a = a\n",
    "        self.e = e\n",
    "        self.env = gym.make(environment, render_mode = \"rgb_array\")\n",
    "\n",
    "        # Initialize the Q-table\n",
    "        # If the environment has a terminal state, set its 0 value to zero\n",
    "        self.Q = {}\n",
    "        for i in range(self.env.action_space.n):\n",
    "            self.Q[(\"done\", i)] = 0.0\n",
    "\n",
    "    def learn(self, n_steps):\n",
    "        \"\"\" \n",
    "        This method is called to run Q-learning for n_steps time steps\n",
    "        \"\"\"\n",
    "\n",
    "        # Create local copies of Q-learning parameters\n",
    "        g = self.g\n",
    "        a = self.a\n",
    "        e = self.e\n",
    "\n",
    "        # Start a new episode and loop n_steps\n",
    "        done = True\n",
    "        for k in range(n_steps):\n",
    "            obs = self.env.reset()[0]\n",
    "\n",
    "            # If this state is not yet in the Q-table, add it and \n",
    "            # initialize values to zero\n",
    "            for i in range(self.env.action_space.n):\n",
    "                if (obs, i) not in self.Q:\n",
    "                    self.Q[(obs, i)] = 0.0\n",
    "\n",
    "        # Select an action with epsilon-greedy action selection\n",
    "        if random.random() < e:\n",
    "            action = self.env.action_space.sample()\n",
    "        else:\n",
    "            _, action = max(\n",
    "                (self.Q[(obs, i)], i) for i in range(self.env.action_space.n)\n",
    "            )\n",
    "            # Apply the selected action and observe the reward and next state\n",
    "            obs_prev = obs\n",
    "            obs, reward, done, info, other = self.env.step(action)\n",
    "\n",
    "            # Indicate whether the episode reached the terminal state\n",
    "            if done is True:\n",
    "                obs = \"done\"\n",
    "\n",
    "            # If the next state is not yet in the Q-table, add it and\n",
    "            # initialize value to zero\n",
    "            for i in range(self.env.action_space.n):\n",
    "                if (obs,i) not in self.Q:\n",
    "                    self.Q[(obs, i)] = 0.0\n",
    "            \n",
    "            # Update the Q value for the previous state and selected action\n",
    "            maxQ, _ = max(\n",
    "                (self.Q[(obs, i)], i) for i in range(self.env.action_space.n)\n",
    "            )\n",
    "            self.Q[(obs_prev, action)] = (1-a)*self.Q[(obs_prev, action)] + a*(reward + g*maxQ) # smoothing the values\n",
    "        \n",
    "    def close(self):\n",
    "        # Close the environment\n",
    "        self.env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner = QLearner(\"Taxi-v3\")\n",
    "learner.learn(500000)\n",
    "learner.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.patches\n",
    "\n",
    "\n",
    "action_desc = {\n",
    "    0 : \"Move south (down)\",\n",
    "    1 : \"Move north (up)\",\n",
    "    2 : \"Move east (right)\",\n",
    "    3 : \"Move west (left)\",\n",
    "    4 : \"Pickup Passenger\",\n",
    "    5 : \"Drop off passenger\" \n",
    "}\n",
    "\n",
    "# Create the Taxi environment\n",
    "env = gym.make('Taxi-v3', render_mode = \"rgb_array\")\n",
    "\n",
    "# Initilize the environment and draw the current state\n",
    "obs = env.reset()[0]\n",
    "plt.imshow(env.render())\n",
    "plt.show()\n",
    "\n",
    "\n",
    "for i in range(200):\n",
    "\n",
    "    if (obs, 0) not in learner.Q:\n",
    "        action = env.action_space.sample()\n",
    "    else:\n",
    "        _, action = max((learner.Q[(obs, i)],i) for i in range(env.action_space.n))\n",
    "\n",
    "    obs, reward, done, info, other = env.step(action)\n",
    "\n",
    "    display.clear_output(wait=True)\n",
    "    plt.imshow(env.render())\n",
    "\n",
    "    rect = matplotlib.patches.Rectangle(\n",
    "        (150, 0), \n",
    "        250,\n",
    "        75,\n",
    "        facecolor=\"#999999\",\n",
    "        edgecolor=\"#000000\"\n",
    "    )\n",
    "    ax = plt.gca()\n",
    "    ax.add_patch(rect)\n",
    "    plt.text(165, 25, f\"State: {obs}\")\n",
    "    plt.text(165, 45, f\"Action: {action_desc[action]}\")\n",
    "    plt.text(165, 65, f\"Reward: {reward}\")\n",
    "\n",
    "    plt.show()\n",
    "    if done:\n",
    "        obs = env.reset()[0]\n",
    "\n",
    "    time.sleep(0.5)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the learned policy now solves the taxi problem optimally in each run.\n",
    "It is important to note that Q-learning knows nothing a-priori about the overall objective of the task. it simply is selecting actions and observing the resulting immediate rewards and next state. By building up the Q table, we encode a policy that is capable of performing the task\n",
    "\n",
    "___\n",
    "We can apply Q-learning algorithm that we implemented to an entirely different task and still learn a policy for performing that task skillfully\n",
    "\n",
    "Let's applt Q-learning to the Blackjack environment in Gymnasium. In this environment, a player plays a single hand Blackjackagainst the dealer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"Blackjack-v1\", render_mode = \"rgb_array\")\n",
    "obs = env.reset()[0]\n",
    "\n",
    "plt.imshow(env.render())\n",
    "plt.show()\n",
    "print(obs)\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner = QLearner(\"Blackjack-v1\")\n",
    "learner.learn(500000)\n",
    "learner.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to evaluate the policy that we learned, we will play fixed number of hands against the dealer and measure the fraction of hands that we win."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_simualtion(n_games, policy_func, **kwargs):\n",
    "    \"\"\" \n",
    "    Here we provide a reusable function for evaluating Blackjack policies.\n",
    "    For a given policy, this fucntion will run n_games using the provided policy\n",
    "    fucntion and return the fraction of games won.\n",
    "    \"\"\"\n",
    "    env =  gym.make(\"Blackjack-v1\", render_mode = \"rdb_array\")\n",
    "\n",
    "    wins = 0\n",
    "    for i in range(n_games):\n",
    "\n",
    "        obs = env.reset()[0]\n",
    "\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            action = policy_func(obs, **kwargs)\n",
    "            obs, reward, done, truncated, info = env.step(action)\n",
    "\n",
    "        if reward > 0:\n",
    "            wins += 1\n",
    "    env.close()\n",
    "    return wins/n_games\n",
    "\n",
    "def Q_policy(state, Q):\n",
    "    \"\"\"\n",
    "    This function implements a Blackjack policy from\n",
    "    a given Q function.\n",
    "    \"\"\"\n",
    "    \n",
    "    if (state, 0) not in Q:\n",
    "        return env.action_space.sample()\n",
    "    elif Q[(state, 0)] > Q[(state, 1)]:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_simualtion(50000, Q_policy, Q = learner.Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9(torch cuda)",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
